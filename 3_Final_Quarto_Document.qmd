
---
title: "Final_Quarto_Document"
format: 
  html:
    toc: true
    toc-float: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
    lang: en
    encoding: UTF-8
execute:
  warning: false
  message: false
  echo: false
  output: false
knitr:
  opts_chunk:
    fig.width: 10
    fig.height: 6
---

Hi, this is supposed to be the final report to be submitted, we can start put parts in it already.

# Preparation of Youtube Data (Dongyuan's part)
## Getting data reday for merge, other titles tbd
<br>

```{r}

library(tidyverse)
library(dplyr)
library(ggplot2)
library(scales)
# Load Google Trends data here
trends_long <- read_csv("trends_combined_long.csv")
# getwd()
# Load Youtube Trending Video data here

getwd()
DEvideos <- read_csv("raw_data_from_kaggle/DEvideos.csv")
FRvideos <- read_csv("raw_data_from_kaggle/FRvideos.csv")
JPvideos <- read_csv("raw_data_from_kaggle/JPvideos.csv")
MXvideos <- read_csv("raw_data_from_kaggle/MXvideos.csv")
USvideos <- read_csv("raw_data_from_kaggle/USvideos.csv")
str(DEvideos)
str(FRvideos)
str(JPvideos)
str(MXvideos)
str(USvideos)
```

<br>
```{r}

#### Add a 'country' column and standardize data types for each data frame
DEvideos <- DEvideos %>%
  mutate(
    country = "DE",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

FRvideos <- FRvideos %>%
  mutate(
    country = "FR",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

JPvideos <- JPvideos %>%
  mutate(
    country = "JP",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

MXvideos <- MXvideos %>%
  mutate(
    country = "MX",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

USvideos <- USvideos %>%
  mutate(
    country = "US",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

#### Combine all data frames into a single one
all_videos <- bind_rows(DEvideos, FRvideos, JPvideos, MXvideos, USvideos)

#### Check the structure of the new, combined data frame
str(all_videos)
```
<br>

```{r}

#### Convert the trending_date column to a proper date format
all_videos <- all_videos %>%
  mutate(trending_date = as.Date(trending_date, format = "%y.%d.%m"))

#### Check the structure again to confirm the change
str(all_videos$trending_date)

#### Find the earliest and latest trending dates
earliest_date <- min(all_videos$trending_date)
latest_date <- max(all_videos$trending_date)
earliest_date
latest_date
#### wordcloud nicht vergessen
```
<br>

<br>

```{r}


#### 1. Separate the tags into individual rows
#### The separate_rows() function is perfect for this
tags_per_day <- all_videos %>%
  separate_rows(tags, sep = "\\|")

#### View(tags_per_day)
str(tags_per_day)

#### 2. Group by date and tag, then count occurrences
trending_tags <- tags_per_day %>%
  group_by(trending_date, tags) %>%
  count(sort = TRUE) %>%
  ungroup()
#### View(trending_tags)

#### 3. View the top trending tags
#### You can filter for a specific date or just look at the top overall
head(trending_tags, 20)
unique_tags_df <- trending_tags %>%
  distinct(tags)
#### View(unique_tags_df)

#### the single most-searched tag across all countries and dates
most_searched_tag <- tags_per_day %>%
  group_by(tags) %>%
  count(sort = TRUE) %>%
  ungroup()

#### View(most_searched_tag)

#### Finding the Most Searched Tag for Each Country
most_searched_tags_by_country <- tags_per_day %>%
  group_by(country, tags) %>%
  count(name = "total_appearances") %>%
  slice_max(order_by = total_appearances, n = 1) %>%
  ungroup() %>%
  arrange(country)

#### View(most_searched_tags_by_country)

```
<br>

<br>

```{r}

#### Group by country and category_id and count the number of videos
category_counts <- all_videos %>%
  group_by(country, category_id) %>%
  count() %>%
  ungroup()

#### View the result
head(category_counts)
#### View(category_counts)

#### Create the grouped bar chart
ggplot(category_counts, aes(x = factor(category_id), y = n, fill = country)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Count of Trending Videos by Category and Country",
    x = "Category ID",
    y = "Number of Trending Videos",
    fill = "Country"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```
<br>


<br>
```{r}

#### Filter for the three keywords and create a new 'keyword' column
relevant_videos <- tags_per_day %>%
  filter(grepl("iphone|tesla|rolex", tags, ignore.case = TRUE)) %>%
  mutate(keyword = case_when(
    grepl("iphone", tags, ignore.case = TRUE) ~ "iPhone",
    grepl("tesla", tags, ignore.case = TRUE) ~ "Tesla",
    grepl("rolex", tags, ignore.case = TRUE) ~ "Rolex",
    TRUE ~ "Other" #### This case should ideally not be triggered after filtering
  ))

#### check the structure and a few rows
str(relevant_videos)
head(relevant_videos)

```
<br>

```{r}

#### Aggregate the data by date, country, and keyword
aggregated_youtube_data <- relevant_videos %>%
  group_by(trending_date, country, keyword) %>%
  summarise(
    total_views = sum(views, na.rm = TRUE),
    total_likes = sum(likes, na.rm = TRUE),
    total_comments = sum(comment_count, na.rm = TRUE),
    total_dislikes = sum(dislikes, na.rm = TRUE), #### Remember the warning about missing US data
    video_count = n()
  ) %>%
  ungroup()

#### Check the new, aggregated dataframe
#### View(aggregated_youtube_data)
```

<br>

```{r}

#### Calculate the engagement ratios
aggregated_youtube_data_final <- aggregated_youtube_data %>%
  mutate(
    #### ratios calculations
    like_dislike_ratio = total_likes / (total_likes + total_dislikes), 
    comments_views_ratio = total_comments / total_views,
    likes_views_ratio = total_likes / total_views,
    dislikes_views_ratio = total_dislikes / total_views
  ) %>%
  #### Select and rename columns for a clean final table
  #### Select: choosing which columns to keep and in what order
  select(
    trending_date, 
    country, 
    keyword, 
    total_views, 
    total_likes,
    total_dislikes,
    total_comments, 
    video_count, 
    like_dislike_ratio,
    comments_views_ratio,
    likes_views_ratio,
    dislikes_views_ratio
  )

#### View the final dataframe, ready for merging
#### View(aggregated_youtube_data_final)
```

<br>

```{r}

#  Calculate the Mean of Proportions and Ratios for each Keyword in each Country
mean_ratios_by_country_keyword <- aggregated_youtube_data_final %>%
  group_by(country, keyword) %>%
  summarise(
    avg_like_dislike_ratio = mean(like_dislike_ratio, na.rm = TRUE),
    avg_comments_views_ratio = mean(comments_views_ratio, na.rm = TRUE),
    avg_likes_views_ratio = mean(likes_views_ratio, na.rm = TRUE),
    avg_dislikes_views_ratio = mean(dislikes_views_ratio, na.rm = TRUE),
    .groups = 'drop'
  )

#### View(mean_ratios_by_country_keyword)
#### colnames(aggregated_youtube_data_final)
```
<br>


# Merging data (Youtube Trending Videos and Google Search)
## Other titles tbd
<br>

```{r}

#  Perform the full join
combined_data <- full_join(aggregated_youtube_data_final, trends_long, 
                           by = c("trending_date" = "date", "country", "keyword"))

# Because every day there is a google trend index, but not every day for each tag there is trendy videos
# Replace NA values with 0 for all columns except the keywords
# This handles the days with no trending videos
combined_data_filled_na <- combined_data %>%
  mutate(across(-c(trending_date, country, keyword), ~replace_na(., 0)))

# Add the status column to indicate if a video was trending
combined_data_final <- combined_data_filled_na %>%
  mutate(youtube_status = if_else(total_views == 0,
                                  "no trending video today for this keyword",
                                  "trending video(s) found"))

# Check your final merged dataset
#### View(combined_data_final)
colnames(combined_data_final)

```
# Data Analysis and Visualisation (Dongyuan's part)
## Part 1: The Big Picture - Comparing the Brands

## Google Search Data Comparison
### Graph 1: Overall Search Interest Comparison v1(Line Chart) v2(Facet Chart)
```{r graph1 Google Comparison v1}
#| output: true
### Graph 1: Overall Search Interest Comparison (Line Chart) v1
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits, color = keyword)) +
  geom_line() +
  labs(
    title = "Google Search Interest Over Time",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  )
```
<br>
### Graph 1: Overall Search Interest Comparison (Facet Chart) v2
```{r graph1 Google Comparison v2 Facet}
### Graph 1: Overall Search Interest Comparison (Facet Chart) v2
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "Google Search Interest Over Time by Keyword",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  ) +
  facet_wrap(~ keyword, scales = "free_y")
```

<br>

## Youtube Data Comparison v1(Line Chart) 
### Graph 2: Overall Search Interest Comparison v1 (Line Chart Version)
```{r graph2 v1}
#| output: true
### Graph 2: Overall Search Interest Comparison v1 (Line Chart Version)
# library(ggplot2)
# library(scales)

ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  #### Format dates
  scale_y_continuous(labels = comma)   #### Apply comma formatting to y-axis, no natural number(scales package)

```

<br>

## Youtube Data Comparison v2(Facet Chart)
### Graph 2: Overall Search Interest Comparison v2 (no Facet Version)
```{r graph2 v2}
#| output: true
### Graph 2: Overall Search Interest Comparison v2 (no Facet Version)
ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  #### Format dates
  scale_y_continuous(labels = comma) +   #### Apply comma formatting to y-axis, no natural number(scales package)
  facet_wrap(~ keyword) #### allow the y-axis of each plot to have own independent scale
  
```

<br>

## Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1

```{r graph4 v1}
#| output: true

### Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1
# prepare data, define the iPhone X launch date.
release_date <- as.Date("2017-11-03")
start_date <- release_date - days(30)
end_date <- release_date + days(30)

# filter combined data to get only the iPhone data, 60-day window.
iphone_hype_data <- combined_data_final %>%
  filter(keyword == "iPhone",
         trending_date >= start_date,
         trending_date <= end_date)

# Create scaling factor for hits, after, both views and hits are visible on the same plot.
# common method to create a dual axis in ggplot2.
scaling_factor <- max(iphone_hype_data$total_views, na.rm = TRUE) / max(iphone_hype_data$hits, na.rm = TRUE)

# plot with ggplot2
ggplot(iphone_hype_data) +
  # YouTube views for left y-axis, bar.
  geom_col(aes(x = trending_date, y = total_views), fill = "blue") +
  
  # Google search hits for right y-axis.
  # scale the hits to match the 'total_views'.
  geom_line(aes(x = trending_date, y = hits * scaling_factor), color = "red", size = 1.2) +
  
  # Add the annotation line for the launch date.
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +  # thickness of the line
  # adds a vertical line to plot, useful for annotations, highlight specific day on time-series graph.
  
  # labels and titles.
  labs(
    title = "iPhone X Hype Cycle: Views vs. Search Interest",
    subtitle = paste("60-day window around launch on", format(release_date, "%B %d, %Y")),
    x = "Date"
  ) +
  
  # Configure the dual y-axis.
  scale_y_continuous(
    name = "Total Daily YouTube Views",
    labels = scales::label_number(scale_cut = cut_short_scale()), # The cut_short_scale() function is a helper from the scales package that automatically sets the cutoffs for thousands (k), millions (M), billions (B), etc. 
    sec.axis = sec_axis(~./scaling_factor, name = "Google Search Interest (0-100)")  # sec_axis() a common function for building customized y axis.
    # transformation formula. The . is a placeholder for values on primary y-axis. 
    # ~ is a shorthand function.
  ) 
  scale_y_continuous(labels = comma) +   #### Apply comma formatting to y-axis, no natural number(scales package)
  facet_wrap(~ keyword) #### allow the y-axis of each plot to have own independent scale

```
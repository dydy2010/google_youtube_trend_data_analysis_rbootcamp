
---
title: "Final_Quarto_Document"
format: 
  html:
    toc: true
    toc-float: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
    lang: en
    encoding: UTF-8
execute:
  warning: false
  message: false
  echo: false
  output: true
knitr:
  opts_chunk:
    fig.width: 10
    fig.height: 6
---

Hi, this is supposed to be the final report to be submitted, we can start put parts in it already.
# Preparation of Youtube Data (Dongyuan's part)
## Getting data reday for merge, other titles tbd
<br>

```{r}
library(tidyverse)
library(dplyr)
getwd()
DEvideos <- read_csv("raw_data_from_kaggle/DEvideos.csv")
FRvideos <- read_csv("raw_data_from_kaggle/FRvideos.csv")
JPvideos <- read_csv("raw_data_from_kaggle/JPvideos.csv")
MXvideos <- read_csv("raw_data_from_kaggle/MXvideos.csv")
USvideos <- read_csv("raw_data_from_kaggle/USvideos.csv")
str(DEvideos)
str(FRvideos)
str(JPvideos)
str(MXvideos)
str(USvideos)
```

<br>
```{r}

#### Add a 'country' column and standardize data types for each data frame
DEvideos <- DEvideos %>%
  mutate(
    country = "DE",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

FRvideos <- FRvideos %>%
  mutate(
    country = "FR",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

JPvideos <- JPvideos %>%
  mutate(
    country = "JP",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

MXvideos <- MXvideos %>%
  mutate(
    country = "MX",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

USvideos <- USvideos %>%
  mutate(
    country = "US",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

#### Combine all data frames into a single one
all_videos <- bind_rows(DEvideos, FRvideos, JPvideos, MXvideos, USvideos)

#### Check the structure of the new, combined data frame
str(all_videos)
```
<br>

```{r}
#### Convert the trending_date column to a proper date format
all_videos <- all_videos %>%
  mutate(trending_date = as.Date(trending_date, format = "%y.%d.%m"))

#### Check the structure again to confirm the change
str(all_videos$trending_date)

#### Find the earliest and latest trending dates
earliest_date <- min(all_videos$trending_date)
latest_date <- max(all_videos$trending_date)
earliest_date
latest_date
#### wordcloud nicht vergessen
```
<br>

<br>

```{r}


#### 1. Separate the tags into individual rows
#### The separate_rows() function is perfect for this
tags_per_day <- all_videos %>%
  separate_rows(tags, sep = "\\|")
  

View(tags_per_day)
str(tags_per_day)
#### 2. Group by date and tag, then count occurrences
trending_tags <- tags_per_day %>%
  group_by(trending_date, tags) %>%
  count(sort = TRUE) %>%
  ungroup()
#### View(trending_tags)

#### 3. View the top trending tags
#### You can filter for a specific date or just look at the top overall
head(trending_tags, 20)
unique_tags_df <- trending_tags %>%
  distinct(tags)
#### View(unique_tags_df)

#### the single most-searched tag across all countries and dates
most_searched_tag <- tags_per_day %>%
  group_by(tags) %>%
  count(sort = TRUE) %>%
  ungroup()
  
View(most_searched_tag)

#### Finding the Most Searched Tag for Each Country
most_searched_tags_by_country <- tags_per_day %>%
  group_by(country, tags) %>%
  count(name = "total_appearances") %>%
  slice_max(order_by = total_appearances, n = 1) %>%
  ungroup() %>%
  arrange(country)

View(most_searched_tags_by_country)
```
<br>

<br>

```{r}
#### Group by country and category_id and count the number of videos
category_counts <- all_videos %>%
  group_by(country, category_id) %>%
  count() %>%
  ungroup()

#### View the result
head(category_counts)
View(category_counts)



#### Create the grouped bar chart
ggplot(category_counts, aes(x = factor(category_id), y = n, fill = country)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Count of Trending Videos by Category and Country",
    x = "Category ID",
    y = "Number of Trending Videos",
    fill = "Country"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```
<br>


<br>
```{r}

#### Filter for the three keywords and create a new 'keyword' column
relevant_videos <- tags_per_day %>%
  filter(grepl("iphone|tesla|rolex", tags, ignore.case = TRUE)) %>%
  mutate(keyword = case_when(
    grepl("iphone", tags, ignore.case = TRUE) ~ "iPhone",
    grepl("tesla", tags, ignore.case = TRUE) ~ "Tesla",
    grepl("rolex", tags, ignore.case = TRUE) ~ "Rolex",
    TRUE ~ "Other" #### This case should ideally not be triggered after filtering
  ))

#### check the structure and a few rows
str(relevant_videos)
head(relevant_videos)

<br>
```
<br>

```{r}

#### Aggregate the data by date, country, and keyword
aggregated_youtube_data <- relevant_videos %>%
  group_by(trending_date, country, keyword) %>%
  summarise(
    total_views = sum(views, na.rm = TRUE),
    total_likes = sum(likes, na.rm = TRUE),
    total_comments = sum(comment_count, na.rm = TRUE),
    total_dislikes = sum(dislikes, na.rm = TRUE), #### Remember the warning about missing US data
    video_count = n()
  ) %>%
  ungroup()

#### Check the new, aggregated dataframe
View(aggregated_youtube_data)
```

<br>

```{r}
#### Calculate the engagement ratios
aggregated_youtube_data_final <- aggregated_youtube_data %>%
  mutate(
    #### ratios calculations
    like_dislike_ratio = total_likes / (total_likes + total_dislikes), 
    comments_views_ratio = total_comments / total_views,
    likes_views_ratio = total_likes / total_views,
    dislikes_views_ratio = total_dislikes / total_views
  ) %>%
  #### Select and rename columns for a clean final table
  #### Select: choosing which columns to keep and in what order
  select(
    trending_date, 
    country, 
    keyword, 
    total_views, 
    total_likes,
    total_dislikes,
    total_comments, 
    video_count, 
    like_dislike_ratio,
    comments_views_ratio,
    likes_views_ratio,
    dislikes_views_ratio
  )

#### View the final dataframe, ready for merging
View(aggregated_youtube_data_final)
```

<br>

```{r}
#  Calculate the Mean of Proportions and Ratios for each Keyword in each Country
mean_ratios_by_country_keyword <- aggregated_youtube_data_final %>%
  group_by(country, keyword) %>%
  summarise(
    avg_like_dislike_ratio = mean(like_dislike_ratio, na.rm = TRUE),
    avg_comments_views_ratio = mean(comments_views_ratio, na.rm = TRUE),
    avg_likes_views_ratio = mean(likes_views_ratio, na.rm = TRUE),
    avg_dislikes_views_ratio = mean(dislikes_views_ratio, na.rm = TRUE),
    .groups = 'drop'
  )

View(mean_ratios_by_country_keyword)
#### colnames(aggregated_youtube_data_final)
```
<br>


# Merging data (Youtube Trending Videos and Google Search)
## Other titles tbd
<br>

```{r}

#  Perform the full join
combined_data <- full_join(aggregated_youtube_data_final, trends_long, 
                           by = c("trending_date" = "date", "country", "keyword"))

# Because every day there is a google trend index, but not every day for each tag there is trendy videos
# Replace NA values with 0 for all columns except the keywords
# This handles the days with no trending videos
combined_data_filled_na <- combined_data %>%
  mutate(across(-c(trending_date, country, keyword), ~replace_na(., 0)))

# Add the status column to indicate if a video was trending
combined_data_final <- combined_data_filled_na %>%
  mutate(youtube_status = if_else(total_views == 0,
                                  "no trending video today for this keyword",
                                  "trending video(s) found"))

# Check your final merged dataset
View(combined_data_final)
colnames(combined_data_final)

```
# Data Analysis and Visualisation (Dongyuan's part)
## Part 1: The Big Picture - Comparing the Brands
```{r graph1}
#| output: true
### Graph 1: Overall Search Interest Comparison (Line Chart) v1
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits, color = keyword)) +
  geom_line() +
  labs(
    title = "Google Search Interest Over Time",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  )

### Graph 1: Overall Search Interest Comparison (Facet Chart) v2
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "Google Search Interest Over Time by Keyword",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  ) +
  facet_wrap(~ keyword, scales = "free_y")
```

<br>

## Part 2: [Next Section Title]

```{r graph2}
#| output: true
### Graph 1: Overall Search Interest Comparison (Faceted Version)
library(ggplot2)
library(scales)

ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  #### Format dates
  scale_y_continuous(labels = comma) +   #### Apply comma formatting to y-axis, no natural number(scales package)
  facet_wrap(~ keyword) #### allow the y-axis of each plot to have own independent scale
  
```
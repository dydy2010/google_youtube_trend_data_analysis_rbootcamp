---
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
## Preparation of Youtube Data
## Getting data reday for merge, other titles tbd

# install.packages("ggridges")
# install.packages("rnaturalearthdata")
library(ggridges)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(scales)
library(sf)
library(rnaturalearth)
## Load Google Trends data here
trends_long <- read_csv("trends_combined_long.csv")
# getwd()
## Load Youtube Trending Video data here

getwd()
DEvideos <- read_csv("raw_data_from_kaggle/DEvideos.csv")
FRvideos <- read_csv("raw_data_from_kaggle/FRvideos.csv")
JPvideos <- read_csv("raw_data_from_kaggle/JPvideos.csv")
MXvideos <- read_csv("raw_data_from_kaggle/MXvideos.csv")
USvideos <- read_csv("raw_data_from_kaggle/USvideos.csv")
# str(DEvideos)
# str(FRvideos)
# str(JPvideos)
# str(MXvideos)
# str(USvideos)

```

```{r}
#| include: false
## Add a 'country' column and standardize data types for each data frame
DEvideos <- DEvideos %>%
  mutate(
    country = "DE",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

FRvideos <- FRvideos %>%
  mutate(
    country = "FR",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

JPvideos <- JPvideos %>%
  mutate(
    country = "JP",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

MXvideos <- MXvideos %>%
  mutate(
    country = "MX",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

USvideos <- USvideos %>%
  mutate(
    country = "US",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

## Combine all data frames into a single one
all_videos <- bind_rows(DEvideos, FRvideos, JPvideos, MXvideos, USvideos)

## Check the structure of the new, combined data frame
# str(all_videos)

```

```{r}
#| include: false
## Convert the trending_date column to a proper date format
all_videos <- all_videos %>%
  mutate(trending_date = as.Date(trending_date, format = "%y.%d.%m"))

## Check the structure again to confirm the change
str(all_videos$trending_date)

## Find the earliest and latest trending dates
earliest_date <- min(all_videos$trending_date)
latest_date <- max(all_videos$trending_date)
earliest_date
latest_date

```

```{r}
#| include: false
#| cache: true
## Separate the tags into individual rows
tags_per_day <- all_videos %>%
  separate_rows(tags, sep = "\\|")

## View(tags_per_day)
str(tags_per_day)

## Group by date and tag, then count occurrences
trending_tags <- tags_per_day %>%
  group_by(trending_date, tags) %>%
  count(sort = TRUE) %>%
  ungroup()
## View(trending_tags)

## View the top trending tags
head(trending_tags, 20)
unique_tags_df <- trending_tags %>%
  distinct(tags)
# View(unique_tags_df)

## the single most-searched tag across all countries and dates
most_searched_tag <- tags_per_day %>%
  group_by(tags) %>%
  count(sort = TRUE) %>%
  ungroup()

# View(most_searched_tag)

## Finding the Most Searched Tag for Each Country
most_searched_tags_by_country <- tags_per_day %>%
  group_by(country, tags) %>%
  count(name = "total_appearances") %>%
  slice_max(order_by = total_appearances, n = 1) %>%
  ungroup() %>%
  arrange(country)

# View(most_searched_tags_by_country)

```

```{r}
#| include: false
#| cache: true
## Filter for the three keywords and create a new 'keyword' column
relevant_videos <- tags_per_day %>%
  filter(grepl("iphone|tesla|rolex", tags, ignore.case = TRUE)) %>%
  mutate(keyword = case_when(
    grepl("iphone", tags, ignore.case = TRUE) ~ "iPhone",
    grepl("tesla", tags, ignore.case = TRUE) ~ "Tesla",
    grepl("rolex", tags, ignore.case = TRUE) ~ "Rolex",
    TRUE ~ "Other" #### This case should ideally not be triggered after filtering
  ))

## check the structure and a few rows
# str(relevant_videos)
# head(relevant_videos)

```

```{r}
#| include: false
## Aggregate the data by date, country, and keyword
aggregated_youtube_data <- relevant_videos %>%
  group_by(trending_date, country, keyword) %>%
  summarise(
    total_views = sum(views, na.rm = TRUE),
    total_likes = sum(likes, na.rm = TRUE),
    total_comments = sum(comment_count, na.rm = TRUE),
    total_dislikes = sum(dislikes, na.rm = TRUE), #### Remember the warning about missing US data
    video_count = n()
  ) %>%
  ungroup()

## Check the new, aggregated dataframe
# View(aggregated_youtube_data)

```

```{r}
#| include: false
## Calculate the engagement ratios
aggregated_youtube_data_final <- aggregated_youtube_data %>%
  mutate(
    #### ratios calculations
    like_dislike_ratio = total_likes / (total_likes + total_dislikes), 
    comments_views_ratio = total_comments / total_views,
    likes_views_ratio = total_likes / total_views,
    dislikes_views_ratio = total_dislikes / total_views
  ) %>%
  ## Select and rename columns for a clean final table
  ## Select: choosing which columns to keep and in what order
  select(
    trending_date, 
    country, 
    keyword, 
    total_views, 
    total_likes,
    total_dislikes,
    total_comments, 
    video_count, 
    like_dislike_ratio,
    comments_views_ratio,
    likes_views_ratio,
    dislikes_views_ratio
  )

## View the final dataframe, ready for merging
# View(aggregated_youtube_data_final)

```

```{r}
#| include: false
## Calculate the Mean of Proportions and Ratios for each Keyword in each Country
mean_ratios_by_country_keyword <- aggregated_youtube_data_final %>%
  group_by(country, keyword) %>%
  summarise(
    avg_like_dislike_ratio = mean(like_dislike_ratio, na.rm = TRUE),
    avg_comments_views_ratio = mean(comments_views_ratio, na.rm = TRUE),
    avg_likes_views_ratio = mean(likes_views_ratio, na.rm = TRUE),
    avg_dislikes_views_ratio = mean(dislikes_views_ratio, na.rm = TRUE),
    .groups = 'drop'
  )

## View(mean_ratios_by_country_keyword)
# colnames(aggregated_youtube_data_final)

```

```{r}
#| include: false
## Ensure trending_date is Date type before joining
aggregated_youtube_data_final <- aggregated_youtube_data_final %>%
  mutate(trending_date = as.Date(trending_date, format = "%y.%d.%m"))
## Merging data (Youtube Trending Videos and Google Search)
##  Perform the full join
combined_data <- full_join(aggregated_youtube_data_final, trends_long, 
                           by = c("trending_date" = "date", "country", "keyword"))

## Because every day there is a google trend index, but not every day for each tag there is trendy videos
## Replace NA values with 0 for all columns except the keywords
## This handles the days with no trending videos
combined_data_filled_na <- combined_data %>%
  mutate(across(-c(trending_date, country, keyword), ~replace_na(., 0)))

## Add the status column to indicate if a video was trending
combined_data_final <- combined_data_filled_na %>%
  mutate(youtube_status = if_else(total_views == 0,
                                  "no trending video today for this keyword",
                                  "trending video(s) found"))

## Check your final merged dataset
# View(combined_data_final)
# colnames(combined_data_final)

```

## Analysis and Visualisation
This chapter presents the core of our analytical project. First, We will first explore the datasets, comparing the brands on both platforms' data, and have a big picture overview. Second, we dive into specific brands on google platform, discovering insights from different aspects. Third, to better understand the hidden patterns, we provide also model analysis, fitting linear regression model to the data. Forth, youtube platform specific analysis for brands was performed. And last but not least, by separating tags, we made ***WordClouds*** to visualise what keywords were most asscociated with the brands, to provide new perspective for product improvement and marketing.
## The Big Picture - Comparing the Brands

Google Search Data Comparison Graph 1: Overall Search Interest
Comparison v1(Line Chart)

```{r graph1 Google Comparison v1, fig.width=14, fig.height=6}
#| echo: false
#| output: true
## Graph 1: Overall Search Interest Comparison (Line Chart) v1
## maybe combining the data points on the same day to one data point, avg, same day
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits, color = keyword)) +
  geom_line() +
  labs(
    title = "Google Search Interest Over Time",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  )

```
To visualise overall search interest on google for 3 brands, a graph with 3 brands search interest data was made. In version 1 all brands are in one graph, and following this later is v2 with 3 brands in 3 graphs. Both graphs show the same result, but are suitable for different reading preferences.
Graph 1: Overall Search Interest Comparison (Facet Chart) v2

```{r graph1 Google Comparison v2 Facet, fig.width=14, fig.height=6}
#| echo: false
#| output: true
## Graph 1: Overall Search Interest Comparison (Facet Chart) v2
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "Google Search Interest Over Time by Keyword",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  ) +
  scale_x_date(
    date_labels = "%Y-%m",       # format Year-Month
    date_breaks = "1 month"      # show every month
  ) +
  facet_wrap(~ keyword, scales = "free_y", nrow=3)

```
Interpretation of Google Search Interest Trends

Key observations include:

* iPhone Dominates Search Interest:

"iPhone" consistently maintains the highest search volume, with significant peaks (e.g., ~100 relative interest) likely corresponding to product launches or major announcements (e.g., iPhone X in 2017, iPhone XS in 2018). It suggests sustained consumer curiosity and media hype around Apple.

* Tesla shows moderate but steady interest:

"Tesla" exhibits lower but steady search interest, with spikes in between. This aligns with Tesla’s brand history, vehicle releases (e.g., Model 3 rollout, massive quality scandals), and Elon Musk’s public visibility, we will also research the possible events that happened around that time later.

* Rolex has negligible search presence:

The keyword "Rolex" shows near-zero search interest throughout the period. This indicates lower public interest compared to the other big brands.
Maybe the brand doesn't need hype as a luxury watch brand, otherwise more marketing campaigns could be beneficial.



Graph 2: Overall YouTube Views Comparison (Line Chart) v1
Same as for google search interst, we did overrall comparison for 3 brands on youtube platform with 2 versions of graphs.

```{r graph2 v1, fig.width=10, fig.height=6}
#| echo: false
#| output: true
## Graph 2: Overall YouTube Views Comparison (Line Chart) v1 (no facet) 
# library(ggplot2)
# library(scales)

ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  ## Format dates
  scale_y_continuous(labels = comma)     ## Apply comma formatting to y-axis (scales package)

```
This graps track the daily view counts for videos related to three keywords—iPhone, Roles, and Tesla, in 5 countries.
It indicates a massive disparity in viewership between iPhone and the other two keywords.

Graph 2: Overall YouTube Views Comparison v2 (Facet Version)

```{r graph2 v2, fig.width=14, fig.height=6}
#| echo: false
#| output: true
## Graph 2: Overall YouTube Views Comparison v2 (no Facet Version)
ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  ## Format dates
  scale_y_continuous(labels = comma) +   ## Apply comma formatting to y-axis (scales package)
  facet_wrap(~keyword, scales = "free_y", nrow=3) ## allow the y-axis of each plot to have own independent scale

```
Interpretation of YouTube Daily Views Trends
* iPhone: Viral and Event-Driven Engagement

The keyword "iPhone" shows an astronomical level of views, peaking at nearly 150 million daily views, which dwarfs the other terms. The trend is very volatile, showing a dramatic spike in early March 2018 followed by a sharp decline. 

This peak is hypothetically due to a product launch or major announcement. After researching, the timing suggests that it was driven by the release of the iPhone X (announced Sept. 2017 and launched on 3.Nov. 2017) or more likely. This pattern could suggest that consumers search for youtube reviews, unboxings, tutorials, and software update related to new Apple products. And it lasted about one month.

* Tesla: Consistent Niche Interest

Trend: The keyword "Tesla" shows a stable trend, ranging from 1 million to 3 million daily views. It shows gentle fluctuations but no sharp, viral spikes like the iPhone.

The steadier trend indicates a noticeable presence on the platform, based on continuous engagement. However certain events could influence the temporary interest of the public. For example, on 17. Nov 2017, Tesla announced their semi truck project, the futuristic design was followed with huge media coverage, which possibly cause a spike of youtube views that lasted about 3 months. And in early 2018 there was huge discussions about Tesla falls short on Model 3 deliveries. In the same period, the stock price droped approx. 33%. This indicates that both launch events and negative coverage could boost Tesla's publicity on youtube.





Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1 
210-day window around the iPhone X launch.

```{r graph4 v1}
#| output: true
#| echo: false
## Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1
## prepare data, define the iPhone X launch date.
release_date <- as.Date("2017-11-03")
start_date <- release_date - days(30)
end_date <- release_date + days(180)

## filter combined data to get only the iPhone data, 60-day window.
iphone_hype_data <- combined_data_final %>%
  filter(keyword == "iPhone",
         trending_date >= start_date,
         trending_date <= end_date)

## Create scaling factor for hits, after, both views and hits are visible on the same plot.
## common method to create a dual axis in ggplot2.
scaling_factor <- max(iphone_hype_data$total_views, na.rm = TRUE) / max(iphone_hype_data$hits, na.rm = TRUE)

## plot with ggplot2
ggplot(iphone_hype_data) +
  ## YouTube views for left y-axis, bar.
  geom_col(aes(x = trending_date, y = total_views), fill = "blue") +
  
  ## Google search hits for right y-axis.
  ## scale the hits to match the 'total_views'.
  geom_smooth(aes(x = trending_date, y = hits * scaling_factor), color = "red", size = 1.2) +
  
  ## Add the annotation line for the launch date.
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  
  ## labels and titles.
  labs(
    title = "iPhone X Hype Cycle: Views vs. Search Interest",
    subtitle = paste("60-day window around launch on", format(release_date, "%B %d, %Y")),
    x = "Date"
  ) +
  
  ## Configure the dual y-axis.
  scale_y_continuous(
    name = "Total Daily YouTube Views",
    #### Using label_number with scale and suffix to format the large nums
    labels = scales::label_number(scale = 1e-6, suffix = "M"), 
    sec.axis = sec_axis(~./scaling_factor, name = "Google Search Interest (0-100)")
  ) +
  
  ## Theme settings.
  theme(
    legend.position = "none",
    ## Increase the aspect ratio, make the plot less squished
    ## adjusts the height-to-width ratio. value less than 1 will make the plot wider
    aspect.ratio = 0.5,
    plot.title = element_text(hjust = 0),
    plot.subtitle = element_text(hjust = 0)
  )

```


Graph 4: The iPhone Hype Cycle (Separate Chart) v2 
210-day window around
the iPhone X launch.

```{r graph4 v2}
#| output: true
#| echo: false
## Graph 4: The iPhone Hype Cycle (Seperate Chart) v2
## prepare data, define the iPhone X launch date.
## layout 1 row and 2 columns
par(mfrow = c(1, 2))

## Plot the Google Trends data
plot_google <- ggplot(iphone_hype_data) +
  geom_smooth(aes(x = trending_date, y = hits), color = "red", size = 1.2, se = FALSE) + ### se=FALSE,standard error. geom_smooth() plots a line, default, it also adds a shaded area around. 
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  labs(title = "Google Search Interest", x = "Date", y = "Relative Interest")

## Plot the YouTube Views data
plot_youtube <- ggplot(iphone_hype_data) +
  geom_col(aes(x = trending_date, y = total_views), fill = "blue") +
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  labs(title = "YouTube Daily Views", x = "Date", y = "Total Daily Views") +
  scale_y_continuous(labels = scales::comma) 
## print needed with par(mfrow)
print(plot_google)
print(plot_youtube)
## good practice reset to default
par(mfrow = c(1, 1))
```

## Google Search Trend Analysis

### Loading and preparing the data

Because downloading Google Trends data is unreliable (more on this later
in our *chapter of choice* about `gtrendsR`), we are working here with
data that has been downloaded previously and saved as `csv` files. All
five country files must be prepared in the same way, for which we have
written a **function**. With `rbind`, we can easily concatenate the
pre-prepared data and use `pivot_longer` to create a second *long* data
set we will need for certain visualisations. With these steps we are now
prepared for the following analyses.

```{r echo=FALSE}
## Install/Load libraries
library(tidyverse)
library(lubridate)
library(sf)
library(rnaturalearth)
library(gtrendsR)  # Downloading was blocked after 3 downloads
library(countrycode)

```

```{r prepare and merge data manually downloaded from Google Trends}
#| include: false

## import and join Google Trends csv datasets

## Function to read and wrangle csv datasets
process_trends_csv <- function(filepath, country_code) {
  ## read csv and skip first two rows
  data <- read.csv(filepath, skip = 2, stringsAsFactors = FALSE)
  
  ## set uniform column names
  colnames(data) <- c("date", "iPhone", "Tesla", "Rolex")

  ## set all <1 values in trend columns to 0
  trend_columns <- c("iPhone", "Tesla", "Rolex")
  data <- data %>%
    mutate(across(all_of(trend_columns),
                  ~ as.numeric(str_replace(., "<1", "0"))))
    
  ## convert date into date format
  data$date <- as.Date(data$date)
  
  ## add column with country_code
  data$country <- country_code
  
  return(data)
  }

```

```{r Alle drei CSV-Dateien einlesen}

#| include: false

## read all data
trends_US <- process_trends_csv("./raw_data_from_google/multiTimeline_US.csv", "US")
trends_MX <- process_trends_csv("./raw_data_from_google/multiTimeline_MX.csv", "MX")
trends_DE <- process_trends_csv("./raw_data_from_google/multiTimeline_DE.csv", "DE")
trends_FR <- process_trends_csv("./raw_data_from_google/multiTimeline_FR.csv", "FR")
trends_JP <- process_trends_csv("./raw_data_from_google/multiTimeline_JP.csv", "JP")

## join all countries and sort by date and country
trends_combined <- rbind(trends_US, trends_MX, trends_DE, trends_FR, trends_JP) %>%
  arrange(date, country)

## Investigate the data
# str(trends_combined)
# head(trends_combined)
# View(trends_combined)
# summary(trends_combined)
# table(trends_combined$country)

## make data long for some visualisations
trends_long <- trends_combined %>%
  pivot_longer(
    cols = c(`iPhone`, `Tesla`, `Rolex`),
    names_to = "keyword",
    values_to = "hits"
  )

# head(trends_long, 10)
# View(trends_long)

```

```{r Speichern der kombinierten Daten}
#| include: false

# write.csv(trends_combined, "trends_combined_wide.csv", row.names = FALSE)
# write.csv(trends_long, "trends_combined_long.csv", row.names = FALSE)

```

**Data inspection**: Besides `head()`, `View()`, `summary()` and
`table()` we use a **faceted line chart** to check whether the data for
all countries and for the three keywords iPhone, Rolex, and Tesla has
been imported correctly.

```{r Facet line chart for all combinations of country & keyword}
#| output: true
#| include: false

## Make the line graph with facets for all combinations

trends_long %>%
  ggplot(aes(x = date, y = hits)) +
  geom_line(colour = "darkblue", linewidth = .6) +
  labs(
    title = "Google Trends searches",
    y = "Relative importance on that day",
    caption = "Source: Google Trends (2017-11-14 to 2018-06-14)"
  ) +
  facet_wrap(~ country + keyword, nrow = 5, ncol = 3) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```

### Correlations between brands

We examine if there is a correlation between **Google Searches for
iPhone and Tesla**. Are days when iPhone searches are very prominent
also days when Tesla searches account for a significant proportion? Or
do searches for these two brands behave independently of each other?
This question may be more relevant for other examples, such as two
**brands in the same industry**. Or when a brand is looking for a
partner for **joint marketing activities**. In such cases, it can be
very helpful to know whether such correlations exist.

Visually, it appears that Google searches for iPhone and Tesla are
independent of each other. We believe this is an expected result.

```{r Correlations in Google search between brands, echo=FALSE, message=FALSE}

## Make the scatter plot with fitted lm line
ggplot(trends_combined, aes(x = iPhone, y = Tesla)) +
  geom_point(alpha = 0.6, size = 2, color = "darkblue") +
  geom_smooth(method = 'lm', se = TRUE) +
  labs(
    title = "Correlation between iPhone and Tesla in Google Trends searches",
    x = "iPhone",
    y = "Tesla",
    caption = "Source: Google Trends. Each point represents a day from 2017-11-14 to 2018-06-14."
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```

### Seasonality of Google searches

An important aspect of analyzing Google Trends data is **seasonality**.
Are there periods when a particular brand is searched for more
frequently? In the case of **Rolex**, for example, the annual
**Baselworld** trade fair at the end of March or the **holiday season**
at the end of the year could result in increased searches on Google.

```{r Rolex bar chart with seasonality of Google Trends}
#| output: true
#| include: false

## Get and prepare the data

# res <- gtrends(
#   keyword = "Rolex",
#   geo     = "",
#   time    = "2018-01-01 2018-12-31")
# 
# write.csv(res$interest_over_time, "./raw_data_from_google/gtrends_Rolex_all_countries_year_2018.csv")

google_Rolex_2018 <- read.csv("./raw_data_from_google/gtrends_Rolex_all_countries_year_2018.csv")

plot_data <- google_Rolex_2018 %>% 
  select(date, hits) %>% 
  filter(date > "2018-01-01")

## Prepare month axis
# plot_data <- plot_data %>%
#   mutate(month_abbr = factor(month(month, label = TRUE, abbr = TRUE),
#                             levels = c("Nov", "Dez", "Jan", "Feb", "Mär", "Apr", "Mai", "Jun")))

## Make the bar chart

ggplot(plot_data, aes(x = date, y = hits)) +
  geom_col(fill = "darkblue") +
  labs(
    title = "Rolex searches worldwide 2018 by week",
    x = "Weeks 2018",
    y = "Relative search interest (100 for month with most searches",
    caption = "Source: Google Trends (2018-01-01 to 2018-12-31)"
  ) +
  scale_y_continuous(limits = c(0, 100)) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),  # Remove x-axis text labels
    axis.ticks.x = element_blank()   #### Remove x-axis ticks
  )

```

The weekly bar chart actually shows these **two peaks**. The first few
weeks of the year, possibly as a echo from the festive season, are
relatively strong, before a first peak follows in **March** after a few
weaker weeks. And in the penultimate week of the year, during the
**holiday season**, the maximum is reached. Because no other brands
relativize Rolex's values, Google Trend sets Rolex's maximum value at
100.

## Model Analysis: fitting a linear regression model and visualize it

### Data visualization

We want to find out how Google Searches for 'iPhone' in different
countries are related. Are days with many searches for iPhone in the US
also days with many searches in Germany? We would expect so as iPhone is
a global topic and news, e.g. the launch of a new iPhone version, swap
across the globe in no time. But still, let's check if we can proof it
with the data.

First we do a line graph comparing the Google Trends data for iPhone
country by country. To get a nicer graph we aggregate the data from
daily to weekly.

```{r echo=FALSE, message=FALSE}

library(tidyverse)
library(corrplot)
# library(patchwork)

trends_combined <- read.csv("trends_combined_wide.csv")

## Prepare data for weekly line graph
trends_combined_week <- trends_combined %>%
  mutate(week = floor_date(ymd(date), "week")) %>% 
  group_by(week, country) %>% 
  summarise(iPhone = mean(iPhone, na.rm = TRUE),
            .groups = 'drop')          

## Make the weekly line graph
ggplot(data = trends_combined_week, aes(x = week, y = iPhone, color = country)) +
  geom_line() +
  labs(
    title = "Google Search Interest Over Time",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Country"
  )

```

As we can see the relative search interest in 'iPhone' is moving quite
similar in all five countries. Now we want do check with a model how
close two countries are related. We pick US and Germany as we are most
interested in these countries. As they are also behaving very similar
across time we expect a high correlation and a strong linear model
explaining the interest in Germany with the interest in the US.

### Prepare the data for modelling and checking all correlations

As initial step we have to prepare the data. We use the daily data as we
want to have data as rich as possible for modelling. For the linear
regression model `lm()` we need separate columns for iPhone search
interest per country. As a first analysis we calculate all correlations
and visualize them with `corrplot`. Especially the correlations between
iPhone_DE and iPhone_US are close to 1 (0.93).

```{r}
#| include: false

## Prepare data for the modelling
lm_data <- trends_combined %>%
  select(date, country, iPhone) %>% 
  pivot_wider(names_from = country, values_from = iPhone, names_prefix = "iPhone_")

## Calculate the correlation coefficients
correlation_matrix <- lm_data %>%
  select(where(is.numeric)) %>%
  cor(use = "complete.obs")

## Look at the correlations
corrplot(correlation_matrix, method = "number", type = "upper",
         tl.col = "black", tl.srt = 45, mar = c(1, 0, 1, 0))

```

### Fitting a linear model

With the data already prepared the fitting of a linear model is straight
forward and with `summary()` we get all relevant information about the
fit.

```{r}

## Fit the linear regression model and show a summary
model <- lm(iPhone_DE ~ iPhone_US, data = lm_data)
summary(model)

```

As expected the linear regression model shows a strong relation between
the Google search interest in iPhone between the US and Germany: The
p-value of the model is close to 0 and therefore significant. The same
is true for the iPhone_US estimate of 0.878. This means an increase of
interest by 1 in the US goes along with an increase of interest in
Germany by 0.878.

### Visualize the linear regression and the quality of the model

We do four visualizations for the model. First a scatter plot showing
all data points (i.e. days) with iPhone_US on the x and iPhone_DE on the
y axis. We add a line for the perfect correlation (y = x) and the
regression line calculated with the model. Therefore we extract
intercept and slope from the coefficients that we received when fitting
the model.

Then we do three standard graphs that are used to check the quality of a
linear regression model:

-   Residuals vs. Fitted to check linearity and constant variance: Any
    clear pattern indicates an issue.
-   Q-Q-plot of Residuals to check the normal distribution of errors.
    Points should follow the line.
-   Observed vs. Fitted to check visually how good model accuracy and
    predictive power are.

```{r, message=FALSE}
#| include: false
#| output: true

## Add fitted values & residuals
lm_data$fitted <- fitted(model)
lm_data$residuals <- residuals(model)

## Extract coefficients for regression line
coef_intercept <- coef(model)[1]
coef_slope <- coef(model)[2]

## Scatterplot with regression line (from model)
plot_1 <- ggplot(lm_data, aes(x = iPhone_US, y = iPhone_DE)) +
  geom_point(alpha = 0.7, size = 2, color = "darkblue") +
  geom_abline(
    aes(intercept = 0, slope = 1, color = "Perfect Correlation (y = x)"),
    linetype = "dashed", alpha = 0.7) +
  geom_abline(
    aes(intercept = coef_intercept, slope = coef_slope, color = "Regression Line"),
    size = 1) +
  scale_color_manual(
    name = "Reference line:",
    values = c("Perfect Correlation (y = x)" = "red",
               "Regression Line" = "blue")) +
  labs(
    title = "Google searches Germany vs USA", ,
    x = "iPhone USA",
    y = "iPhone Germany",
    caption = "Source: Google Trends. Each point represents a day from 2017-11-14 to 2018-06-14.") +
  theme_minimal() +
  theme(legend.position = "bottom"
  )

## Residuals vs Fitted
plot_2 <- ggplot(lm_data, aes(fitted, residuals)) +
  geom_point(alpha = 0.7, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted", x = "Fitted", y = "Residuals") +
  theme_minimal()

## Q-Q Plot
plot_3 <- ggplot(lm_data, aes(sample = residuals)) +
  stat_qq(alpha = 0.7, color = "darkblue") +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

## Observed vs Fitted
plot_4 <- ggplot(lm_data, aes(fitted, iPhone_DE)) +
  geom_point(alpha = 0.7, color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Observed vs Fitted", x = "Fitted", y = "Observed") +
  theme_minimal()

## Arrange plots
# (plot_1 | plot_2) / (plot_3 | plot_4)

plot_1
plot_2
plot_3
plot_4

```

To keep the story short: There are no major issues with the model. It is
explaining the Google search interest in Germany very well with the
Google search interest in the US. Therefore the result of the analysis
confirms our initial expectation.

### Conclusion for our clients

For any brand it is important to know the level of interest (potential)
clients show over time. Especially when a major event is planned a
monitoring of Google Search should be set up to understand how
effectively the event triggered interest, how it developed afterwards
and how the interest was in the markets/countries the brand is most
interested in.

In some countries, the event may have caused a sharp increase in
interest that will last for a long time. While in other countries the
effect may be rather short-lived. It is essential to be aware of such
effects in order to respond appropriately with marketing measures.

## Youtube Videos Analysis

Scatterplot Tesla Youtube-views vs. Youtube-likes

```{r echo=FALSE, message=FALSE}
#| output: true

## Prepare data

plot_data <- aggregated_youtube_data_final %>% 
  filter(keyword == 'Rolex') %>% 
  select(trending_date, total_views, total_likes) %>% 
  mutate(engagement = total_likes / total_views)

## Plot the scatter plot

ggplot(plot_data, aes(x = total_views, y = engagement)) +
  geom_point(alpha = 0.6, size = 2, color = "darkblue") +
  scale_x_continuous(labels = scales::number, expand = expansion(mult = c(0, 0.1))) +
  geom_smooth(method = 'lm', se = FALSE) +
  labs(
    title = "Correlation between engagement with and views of Rolex YouTube videos",
    x = "Total Views",
    y = "Engagement: Likes / Views",
    caption = "Source: Google Trends. Each point represents a day from 2017-11-14 to 2018-06-14."
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```

Barchart Brand comparison of YouTube-Engagement (i.e. likes / views and
dislikes / views)

```{r echo=FALSE, message=FALSE}

## Prepare data

plot_data <- aggregated_youtube_data_final %>% 
  select(trending_date, keyword, total_views, total_likes, total_dislikes) %>% 
  group_by(keyword) %>% 
  summarise(total_views = sum(total_views),
            total_likes = sum(total_likes),
            total_dislikes = sum(total_dislikes)) %>% 
  mutate(like_ratio = total_likes / total_views,
         dislike_ratio = total_dislikes / total_views) %>% 
  mutate(keyword = factor(keyword, levels = c('iPhone', 'Tesla', 'Rolex')))

## Make the column plot

# ggplot(plot_data,aes(x = keyword, y = like_ratio)) +
#   geom_col(fill = "darkblue") +
#   geom_text(aes(label = sprintf("%.3f", like_ratio)),
#             vjust = -0.5, size = 3.2) +
#   scale_y_continuous(limits = c(0, 0.05)) +
#   theme_minimal() +
#   labs(
#     title = "Brand comparison of YouTube video engagement",
#     x = "Keywords",
#     y = "Engagement (total likes / total views)",
#     caption = "Source: Kaggle dataset Youtube (November 2017 to Juin 2018)"
#   )

## Make a combined column plot for like- and dislike-ratio

## Convert to long format: one column for metric type, one for value
plot_long <- plot_data %>%
  pivot_longer(cols = c(like_ratio, dislike_ratio),
               names_to = "metric",
               values_to = "ratio")

## Plot with grouped bars
ggplot(plot_long, aes(x = keyword, y = ratio, fill = metric)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = sprintf("%.3f", ratio)),
            vjust = -0.5, size = 3.2,
            position = position_dodge(width = 0.9)) +
  scale_y_continuous(limits = c(0, 0.05)) +
  scale_fill_manual(values = c("like_ratio" = "darkblue",
                               "dislike_ratio" = "firebrick")) +
  theme_minimal() +
  labs(
    title = "Brand comparison of YouTube video engagement",
    x = "Keywords",
    y = "Engagement (likes/dislikes per views)",
    fill = "Metric",
    caption = "Source: Kaggle dataset Youtube (Nov 2017 – Jun 2018)"
  )

```

Longevity of selected YouTube-videos

```{r echo=FALSE, message=FALSE}
#| output: true
## Prepare data

plot_data <- relevant_videos %>% 
  select(video_id, trending_date, views, keyword) %>% 
  group_by(video_id, trending_date, keyword) %>% 
  summarise(total_views = sum(views))

# plot_data %>% 
#   group_by(keyword) %>%
#   slice_max(total_views, n = 1) %>%
#   ungroup()

plot_data <- plot_data %>% 
  filter(video_id %in% c("RgpoRGq3oBs",	"WpqUOW19aJQ", "2fGXDFiFBhg"))

## Make the line graph

plot_data %>%
  ggplot(aes(x = trending_date, y = total_views, colour = keyword)) +
  geom_line(linewidth = .6) +
  labs(
    title = "YouTube most watched videos: Development over time",
    y = "Relative importance on that day",
    caption = "Source: Google Trends (2017-11-14 to 2018-06-14)"
  ) +
  theme_minimal()

```

## WordCloud

After all these analyses and visualizations of basically 'numbers' we
should not forget that brands and branding are primarily about
**content.** Here we have good news: YouTube and Google Searches are
too! One should and can examine what the actual content of the videos
watched in YouTube is, besides the fact that they are tagged with the
keywords 'Rolex' or 'iPhone'. With a wordcloud we can give a very good
first impression of the content based on all tags that were given to the
videos. We believe knowing what the content of the videos is, is as
important as knowing how often they have been watched.

First we do a wordcloud for **Rolex**. This means: We are counting all
tags given to all Rolex videos. As a reminder: A Rolex video is a video
that contains Rolex in the title or in the tags. Then we select the most
frequent tags (besides Rolex) and arrange them in a form that usually is
a kind of a flat circle. The most frequent tags/words are placed first
in the center and with a larger font than the less frequent words that
get a smaller font and are placed later where there is empty space to
put them.

The R package `wordcloud2` provides many parameters which allow to
define a very specific look of the wordcloud. Without going to much into
detail **the process has two steps**:

1.  You have to prepare the words to be displayed in a **dataframe with
    two columns**: the words and their counts. Regarding the words some
    cleaning is needed like removing white spaces and stopwords (words
    with no relevance for the wordcloud like 'and' etc. ). And when you
    see the first wordcloud you will most likely remove other words that
    you don't want to be shown or words that are just written
    differently that you want to combine.
2.  Then you call the `wordcloud2()` function and define all **visual
    parameters**. This step requires a trial and error approach to set
    all size parameters correctly to get a nice wordcloud. The
    documentation of the package is good, so you know which parameter
    does what. But still you will be surprised how minor changes make
    the wordcloud look good or rather useless.

Now follows the preparation of the data. Short note: Some packages
loaded here are not necessary for the wordcloud but allow to e.g. check
the fonts that you can use (`systemfonts`) or to export the wordcloud
image as pdf (`webshot2`and `htmlwidgets`).

```{r echo=FALSE}

library(ragg)
library(systemfonts)
library(textshaping)

# View(systemfonts::system_fonts())
## Test Fonts. Use family name!

library(RColorBrewer)
library(wordcloud2)
library(tidyverse)
library(stopwords)
library(webshot2)
library(htmlwidgets)

```

### Rolex wordcloud

```{r Prepare the Rolex data}
#| include: false

## Use only videos that have Rolex in the title or in the tags
rolex_df <- all_videos %>% 
  filter(
    str_detect(title, regex("rolex", ignore_case = TRUE)) |
    str_detect(tags, regex("rolex", ignore_case = TRUE))
  )

## Aggregate by video_id (sum views, keep tags)
rolex_df_aggregated <- rolex_df %>%
  group_by(video_id, tags) %>%
  summarise(total_views = sum(views, na.rm = TRUE), .groups = "drop")

## Split tags by "|" separator (creates multiple rows per video)
rolex_tags_split <- rolex_df_aggregated %>%
  separate_rows(tags, sep = "\\|")

## Clean individual tags
rolex_tags_cleaned <- rolex_tags_split %>%
  mutate(
    # Remove leading/trailing whitespace
    tags_clean = str_trim(tags),
    # Convert to lowercase for consistency
    tags_clean = str_to_lower(tags_clean),
    # Remove extra whitespace within tags
    tags_clean = str_squish(tags_clean)
  ) %>%
  # Remove empty tags and stopwords
  filter(
    tags_clean != "",
    !tags_clean %in% stopwords("en"),
    # Remove additional words you do not want to see in the cloud, namely 'Rolex'
    !tags_clean %in% c("video", "youtube", "content", "2017", "2018", "olexesh", "rolex")
  ) %>%
  # Keep only the cleaned tags column
  select(tag = tags_clean, total_views)

## View results
# head(rolex_tags_cleaned)
# cat("Total unique clean tags:", n_distinct(rolex_tags_cleaned$tag), "\n")

## aggregate to have unique tags
rolex_tags_cleaned_unique <- rolex_tags_cleaned %>%
  group_by(tag) %>%
  summarise(total_views = sum(total_views, na.rm = TRUE)) %>%
  arrange(desc(total_views))

## View results
# head(rolex_tags_cleaned_unique, 100)
```

Then we create the Rolex wordcloud. We restrict the data to words with
more than 100k views and set a specific palette that represents the
corporate identity of Rolex. And we set the font to Garamond which is
close to the font Rolex uses.

```{r}
#| output: true
## Define luxurious colors that match to the Rolex brand.

pal_rolex <- c("#228B22", "#0A1931", "#B09770", "#722F37", "#B36A5E", "#454545", "#006D77", "#4C3F91")

## Create the Rolex wordcloud
wordcloud2(rolex_tags_cleaned_unique %>% filter(total_views > 100000),
           size = 0.3, rotateRatio = 0,
           minRotation = -pi/2, maxRotation = -pi/2,
           fontFamily = 'Garamond',
           fontWeight = 800,
           gridSize = 4, 
           shuffle = TRUE,
           shape = 'circle', ellipticity = 0.5,
           # color = palKH2[factor(df2$Word)])
           color = rep_len(pal_rolex, nrow(rolex_tags_cleaned_unique)))

```

The resulting word cloud is brilliant: it shows that the most-watched
YouTube videos are heavily influenced by the **German hip-hop** scene!
Bushido, Kollegah, Farid Bang, and Nimo are well-known German rappers,
and they clearly like to show off their Rolex watches. We don't think
the folks in charge of Rolex branding are too happy about this. We'd
suggest they try to counteract the German videos. Of course, having the
Rolex brand in hip-hop videos is a good thing in itself. But Rolex
should try to make sure that YouTube videos show other Rolex values
besides just “expensive.”

### iPhone wordcloud

**As a second example**, to give an impression on how diverse wordclouds
can become, we do a wordcloud for **iPhone** with the respective YouTube
tags. We adjust the color palette to the colors Apple used for their
iPhones in 2017/18 and we adjust the font to a type of Helvetica. Here
we have to restrict the number of words to 500k. According to the
package description it would be even possible to **define the form of
the wordcloud based on a picture**. We could use the apple logo (a png
file with the logo in black on a white background) and `wordcloud2` will
try to fill the space used by the Apple logo. So far we did not manage
to get it working, but it is definitely something we will try out the
next time we do a wordcloud.

```{r Prepare the iPhone data}
#| include: false
#| output: true
## Use only videos that have iphone in the title or in the tags
iphone_df <- all_videos %>% 
  filter(
    str_detect(title, regex("iphone", ignore_case = TRUE)) |
    str_detect(tags, regex("iphone", ignore_case = TRUE))
  )

## Aggregate by video_id (sum views, keep tags)
iphone_df_aggregated <- iphone_df %>%
  group_by(video_id, tags) %>%
  summarise(total_views = sum(views, na.rm = TRUE), .groups = "drop")

## Split tags by "|" separator (creates multiple rows per video)
iphone_tags_split <- iphone_df_aggregated %>%
  separate_rows(tags, sep = "\\|")

## Clean individual tags
iphone_tags_cleaned <- iphone_tags_split %>%
  mutate(
    # Remove leading/trailing whitespace
    tags_clean = str_trim(tags),
    # Convert to lowercase for consistency
    tags_clean = str_to_lower(tags_clean),
    # Remove extra whitespace within tags
    tags_clean = str_squish(tags_clean)
  ) %>%
  ## Remove empty tags and stopwords
  filter(
    tags_clean != "",
    !tags_clean %in% stopwords("en"),
    # Remove additional words you do not want to see in the cloud, namely 'iPhone'
    !tags_clean %in% c("video", "youtube", "content", "2017", "2018", "iphone", "iphone x")
  ) %>%
  ## Keep only the cleaned tags column
  select(tag = tags_clean, total_views)

## View results
# head(iphone_tags_cleaned)
# cat("Total unique clean tags:", n_distinct(iphone_tags_cleaned$tag), "\n")

## aggregate to have unique tags
iphone_tags_cleaned_unique <- iphone_tags_cleaned %>%
  group_by(tag) %>%
  summarise(total_views = sum(total_views, na.rm = TRUE)) %>%
  arrange(desc(total_views))

## View results
# head(iphone_tags_cleaned_unique, 100)

```

```{r}
#| include: false
#| output: true

## Define colors that match to Apple and specifically to the iPhone X launched 2017/2018.

pal_iphone <- c("#FF375F", "#32D74B", "#0A84FF", "#FF9F0A", "#BF5AF2", "#1D1D1F", "#E4E4E2")

## iPhone wordcloud

wc_iPhone <- wordcloud2(iphone_tags_cleaned_unique %>% filter(total_views > 5000000),
           size = 0.2, rotateRatio = 0,
           minRotation = -pi/2, maxRotation = -pi/2,
           fontFamily = 'Helvetica Neue LT Std',
           fontWeight = 800,
           gridSize = 4, 
           shuffle = TRUE,
           shape = 'circle', ellipticity = 0.5,
           # color = palKH2[factor(df2$Word)])
           color = rep_len(pal_iphone, nrow(iphone_tags_cleaned_unique)))

wc_iPhone

## Export a png

## Step 1: Save as HTML in a known folder
# html_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_iPhone.html"
# saveWidget(wc_iPhone, file = html_file, selfcontained = TRUE)

## Step 2: Take PNG snapshot of that HTML
# png_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_iPhone.png"
# webshot2::webshot(html_file, png_file, vwidth = 800, vheight = 600)

```

We think this word cloud is much more in line with what iPhone managers
expect. iPhone YouTube videos are tagged with terms from the world of
**smartphones** and the **gaming scene**. Apple itself also seems to
have a strong influence: the terms “unbox therapy” and “unboxing”
probably originate from videos by **influencers** who are paid by Apple
to unbox a new iPhone and share their first impressions with their
followers. But it could be interesting to see how the word cloud
develops thematically when no new iPhone is being launched.

In **summary**, we can say that it is very worthwhile to use word clouds
to get a feel for the content of videos. Major brands such as Apple and
Rolex are already doing this, and there are undoubtedly more advanced
content analysis tools than word clouds. But for **smaller brands** in
particular, it could be a first step toward better understanding the
image their brand projects on YouTube.

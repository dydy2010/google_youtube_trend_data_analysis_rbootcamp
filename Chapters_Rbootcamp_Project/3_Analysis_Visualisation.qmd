
---
title: "Analysis and Visualisation"
format: 
  html:
    toc: true
    toc-float: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
    lang: en
    encoding: UTF-8
execute:
  warning: false
  message: false
  echo: false
  output: false
knitr:
  opts_chunk:
    fig.width: 10
    fig.height: 6
---



```{r}
#### Preparation of Youtube Data (Dongyuan's part)
#### Getting data reday for merge, other titles tbd

#### install.packages("ggridges")
#### install.packages("rnaturalearthdata")
library(ggridges)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(scales)
library(sf)
library(rnaturalearth)
#### Load Google Trends data here
trends_long <- read_csv("trends_combined_long.csv")
#### getwd()
#### Load Youtube Trending Video data here

getwd()
DEvideos <- read_csv("raw_data_from_kaggle/DEvideos.csv")
FRvideos <- read_csv("raw_data_from_kaggle/FRvideos.csv")
JPvideos <- read_csv("raw_data_from_kaggle/JPvideos.csv")
MXvideos <- read_csv("raw_data_from_kaggle/MXvideos.csv")
USvideos <- read_csv("raw_data_from_kaggle/USvideos.csv")
str(DEvideos)
str(FRvideos)
str(JPvideos)
str(MXvideos)
str(USvideos)
```

```{r}

#### Add a 'country' column and standardize data types for each data frame
DEvideos <- DEvideos %>%
  mutate(
    country = "DE",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

FRvideos <- FRvideos %>%
  mutate(
    country = "FR",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

JPvideos <- JPvideos %>%
  mutate(
    country = "JP",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

MXvideos <- MXvideos %>%
  mutate(
    country = "MX",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

USvideos <- USvideos %>%
  mutate(
    country = "US",
    across(c(comments_disabled, ratings_disabled, video_error_or_removed), as.character)
  )

#### Combine all data frames into a single one
all_videos <- bind_rows(DEvideos, FRvideos, JPvideos, MXvideos, USvideos)

#### Check the structure of the new, combined data frame
str(all_videos)
```


```{r}

#### Convert the trending_date column to a proper date format
all_videos <- all_videos %>%
  mutate(trending_date = as.Date(trending_date, format = "%y.%d.%m"))

#### Check the structure again to confirm the change
str(all_videos$trending_date)

#### Find the earliest and latest trending dates
earliest_date <- min(all_videos$trending_date)
latest_date <- max(all_videos$trending_date)
earliest_date
latest_date
#### wordcloud nicht vergessen
```


```{r}


#### 1. Separate the tags into individual rows
#### The separate_rows() function is perfect for this
tags_per_day <- all_videos %>%
  separate_rows(tags, sep = "\\|")

#### View(tags_per_day)
str(tags_per_day)

#### 2. Group by date and tag, then count occurrences
trending_tags <- tags_per_day %>%
  group_by(trending_date, tags) %>%
  count(sort = TRUE) %>%
  ungroup()
#### View(trending_tags)

#### 3. View the top trending tags
#### You can filter for a specific date or just look at the top overall
head(trending_tags, 20)
unique_tags_df <- trending_tags %>%
  distinct(tags)
#### View(unique_tags_df)

#### the single most-searched tag across all countries and dates
most_searched_tag <- tags_per_day %>%
  group_by(tags) %>%
  count(sort = TRUE) %>%
  ungroup()

#### View(most_searched_tag)

#### Finding the Most Searched Tag for Each Country
most_searched_tags_by_country <- tags_per_day %>%
  group_by(country, tags) %>%
  count(name = "total_appearances") %>%
  slice_max(order_by = total_appearances, n = 1) %>%
  ungroup() %>%
  arrange(country)

#### View(most_searched_tags_by_country)

```

```{r}

#### Filter for the three keywords and create a new 'keyword' column
relevant_videos <- tags_per_day %>%
  filter(grepl("iphone|tesla|rolex", tags, ignore.case = TRUE)) %>%
  mutate(keyword = case_when(
    grepl("iphone", tags, ignore.case = TRUE) ~ "iPhone",
    grepl("tesla", tags, ignore.case = TRUE) ~ "Tesla",
    grepl("rolex", tags, ignore.case = TRUE) ~ "Rolex",
    TRUE ~ "Other" #### This case should ideally not be triggered after filtering
  ))

#### check the structure and a few rows
str(relevant_videos)
head(relevant_videos)

```


```{r}

#### Aggregate the data by date, country, and keyword
aggregated_youtube_data <- relevant_videos %>%
  group_by(trending_date, country, keyword) %>%
  summarise(
    total_views = sum(views, na.rm = TRUE),
    total_likes = sum(likes, na.rm = TRUE),
    total_comments = sum(comment_count, na.rm = TRUE),
    total_dislikes = sum(dislikes, na.rm = TRUE), #### Remember the warning about missing US data
    video_count = n()
  ) %>%
  ungroup()

#### Check the new, aggregated dataframe
#### View(aggregated_youtube_data)
```


```{r}

#### Calculate the engagement ratios
aggregated_youtube_data_final <- aggregated_youtube_data %>%
  mutate(
    #### ratios calculations
    like_dislike_ratio = total_likes / (total_likes + total_dislikes), 
    comments_views_ratio = total_comments / total_views,
    likes_views_ratio = total_likes / total_views,
    dislikes_views_ratio = total_dislikes / total_views
  ) %>%
  #### Select and rename columns for a clean final table
  #### Select: choosing which columns to keep and in what order
  select(
    trending_date, 
    country, 
    keyword, 
    total_views, 
    total_likes,
    total_dislikes,
    total_comments, 
    video_count, 
    like_dislike_ratio,
    comments_views_ratio,
    likes_views_ratio,
    dislikes_views_ratio
  )

#### View the final dataframe, ready for merging
#### View(aggregated_youtube_data_final)
```



```{r}

# Calculate the Mean of Proportions and Ratios for each Keyword in each Country
mean_ratios_by_country_keyword <- aggregated_youtube_data_final %>%
  group_by(country, keyword) %>%
  summarise(
    avg_like_dislike_ratio = mean(like_dislike_ratio, na.rm = TRUE),
    avg_comments_views_ratio = mean(comments_views_ratio, na.rm = TRUE),
    avg_likes_views_ratio = mean(likes_views_ratio, na.rm = TRUE),
    avg_dislikes_views_ratio = mean(dislikes_views_ratio, na.rm = TRUE),
    .groups = 'drop'
  )

#### View(mean_ratios_by_country_keyword)
#### colnames(aggregated_youtube_data_final)
```


```{r}

#### Merging data (Youtube Trending Videos and Google Search)
####  Perform the full join
combined_data <- full_join(aggregated_youtube_data_final, trends_long, 
                           by = c("trending_date" = "date", "country", "keyword"))

#### Because every day there is a google trend index, but not every day for each tag there is trendy videos
#### Replace NA values with 0 for all columns except the keywords
#### This handles the days with no trending videos
combined_data_filled_na <- combined_data %>%
  mutate(across(-c(trending_date, country, keyword), ~replace_na(., 0)))

#### Add the status column to indicate if a video was trending
combined_data_final <- combined_data_filled_na %>%
  mutate(youtube_status = if_else(total_views == 0,
                                  "no trending video today for this keyword",
                                  "trending video(s) found"))

#### Check your final merged dataset
#### View(combined_data_final)
colnames(combined_data_final)
```


## The Big Picture - Comparing the Brands
   Google Search Data Comparison
   Graph 1: Overall Search Interest Comparison v1(Line Chart)
```{r graph1 Google Comparison v1, fig.width=14, fig.height=6}
#| output: true
#### Graph 1: Overall Search Interest Comparison (Line Chart) v1
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits, color = keyword)) +
  geom_line() +
  labs(
    title = "Google Search Interest Over Time",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  )
```
<br>
Graph 1: Overall Search Interest Comparison (Facet Chart) v2
```{r graph1 Google Comparison v2 Facet, fig.width=14, fig.height=6}
### Graph 1: Overall Search Interest Comparison (Facet Chart) v2
library(ggplot2)

ggplot(data = combined_data_final, aes(x = trending_date, y = hits)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "Google Search Interest Over Time by Keyword",
    x = "Date",
    y = "Relative Search Interest (0-100)",
    color = "Keyword"
  ) +
  scale_x_date(
    date_labels = "%Y-%m",       # format Year-Month
    date_breaks = "1 month"      # show every month
  ) +
  facet_wrap(~ keyword, scales = "free_y", nrow=3)
```

<br>

Graph 2: Overall YouTube Views Comparison (Line Chart) v1 
```{r graph2 v1, fig.width=10, fig.height=6}
#| output: true
### Graph 2: Overall YouTube Views Comparison (Line Chart) v1 (no facet) 
#### library(ggplot2)
#### library(scales)

ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  #### Format dates
  scale_y_continuous(labels = comma)   #### Apply comma formatting to y-axis, no natural number(scales package)

```

<br>

Graph 2: Overall YouTube Views Comparison v2 (Facet Version)
```{r graph2 v2, fig.width=14, fig.height=6}
#| output: true
#### Graph 2: Overall YouTube Views Comparison v2 (no Facet Version)
ggplot(data = combined_data_final, aes(x = trending_date, y = total_views)) +
  geom_line(aes(color = keyword)) +
  labs(
    title = "YouTube Daily Views Over Time",
    x = "Date",
    y = "Total Daily Views",
    color = "Keyword"
  ) +
  scale_x_date(date_labels = "%b %Y") +  #### Format dates
  scale_y_continuous(labels = comma) +   #### Apply comma formatting to y-axis, no natural number(scales package)
  facet_wrap(~keyword, scales = "free_y", nrow=3) #### allow the y-axis of each plot to have own independent scale
```
<br>
Graph 3: Distribution of Views v1 Box Plot, doesnt work well with single data point per day somehow
Compare the distribution of daily views for Tesla vs. iPhone vs. Rolex videos.
```{r graph3 v1}
#| output: true
ggplot(data = combined_data_final, aes(x = keyword, y = total_views)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Daily YouTube Views",
    subtitle = "Comparing Engagement for iPhone, Tesla, and Rolex",
    x = "Keyword",
    y = "Daily Views"
  ) +
  scale_y_continuous(labels = scales::comma) #### Apply comma formatting to y-axis, no natural number(scales package)

```

<br>
Graph 3: Distribution of Engagement v2 Violin Plot, doesnt work well, due to single data point per day somehow
Compare the distribution of daily views for iPhone vs. Rolex videos.
```{r graph3 v2}
#| output: true
ggplot(data = combined_data_final, aes(x = keyword, y = total_views)) +
  geom_violin(aes(fill = keyword), alpha = 0.6) +
  labs(
    title = "Distribution of Daily YouTube Views",
    subtitle = "Comparing Engagement for iPhone, Tesla, and Rolex",
    x = "Keyword",
    y = "Daily Views"
  ) +
  scale_y_continuous(labels = label_number(scale = 1e-6, suffix = "M")) #### Apply comma formatting to y-axis, no natural number(scales package)

```
<br>
Graph 3: Distribution of Engagement v3 Ridge Plot
ridge plot is a modern and visually compelling way to compare distributions. It is especially effective data that shows peaks and valleys
Compare the distribution of daily views for iPhone vs. Rolex videos.
```{r graph3 v3}
#| output: true
#### install.packages("ggridges")
#### library(ggridges)

ggplot(data = combined_data_final, aes(x = total_views, y = keyword, fill = keyword)) +
  geom_density_ridges(alpha = 0.8) +
  labs(
    title = "Distribution of Daily YouTube Views",
    subtitle = "Comparing Engagement for iPhone, Tesla, and Rolex",
    x = "Daily Views",
    y = "Keyword"
  ) +
  scale_x_continuous(labels = scales::comma) +
  theme(
    legend.position = "none",
    #### Increase the aspect ratio, make the plot less squished
    #### adjusts the height-to-width ratio. value less than 1 will make the plot wider
    aspect.ratio = 0.5,
    #### Left-align the titles for better readability
    plot.title = element_text(hjust = 0),
    plot.subtitle = element_text(hjust = 0)
  )
```
<br>

Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1
210-day window around the iPhone X launch.

```{r graph4 v1}
#| output: true
### Graph 4: The iPhone Hype Cycle (Dual-Axis Chart) v1
#### prepare data, define the iPhone X launch date.
release_date <- as.Date("2017-11-03")
start_date <- release_date - days(30)
end_date <- release_date + days(180)

#### filter combined data to get only the iPhone data, 60-day window.
iphone_hype_data <- combined_data_final %>%
  filter(keyword == "iPhone",
         trending_date >= start_date,
         trending_date <= end_date)

#### Create scaling factor for hits, after, both views and hits are visible on the same plot.
#### common method to create a dual axis in ggplot2.
scaling_factor <- max(iphone_hype_data$total_views, na.rm = TRUE) / max(iphone_hype_data$hits, na.rm = TRUE)

#### plot with ggplot2
ggplot(iphone_hype_data) +
  #### YouTube views for left y-axis, bar.
  geom_col(aes(x = trending_date, y = total_views), fill = "blue") +
  
  #### Google search hits for right y-axis.
  #### scale the hits to match the 'total_views'.
  geom_smooth(aes(x = trending_date, y = hits * scaling_factor), color = "red", size = 1.2) +
  
  #### Add the annotation line for the launch date.
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  
  #### labels and titles.
  labs(
    title = "iPhone X Hype Cycle: Views vs. Search Interest",
    subtitle = paste("60-day window around launch on", format(release_date, "%B %d, %Y")),
    x = "Date"
  ) +
  
  #### Configure the dual y-axis.
  scale_y_continuous(
    name = "Total Daily YouTube Views",
    #### Using label_number with scale and suffix to format the large nums
    labels = scales::label_number(scale = 1e-6, suffix = "M"), 
    sec.axis = sec_axis(~./scaling_factor, name = "Google Search Interest (0-100)")
  ) +
  
  #### Theme settings.
  theme(
    legend.position = "none",
    #### Increase the aspect ratio, make the plot less squished
    #### adjusts the height-to-width ratio. value less than 1 will make the plot wider
    aspect.ratio = 0.5,
    plot.title = element_text(hjust = 0),
    plot.subtitle = element_text(hjust = 0)
  )
```


<br>

Graph 4: The iPhone Hype Cycle (Separate Chart) v2
210-day window around the iPhone X launch.

```{r graph4 v2}
#| output: true
### Graph 4: The iPhone Hype Cycle (Seperate Chart) v2
#### prepare data, define the iPhone X launch date.
#### layout 1 row and 2 columns
par(mfrow = c(1, 2))

#### Plot the Google Trends data
plot_google <- ggplot(iphone_hype_data) +
  geom_smooth(aes(x = trending_date, y = hits), color = "red", size = 1.2, se = FALSE) + ### se=FALSE,standard error. geom_smooth() plots a line, default, it also adds a shaded area around. 
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  labs(title = "Google Search Interest", x = "Date", y = "Relative Interest")

#### Plot the YouTube Views data
plot_youtube <- ggplot(iphone_hype_data) +
  geom_col(aes(x = trending_date, y = total_views), fill = "blue") +
  geom_vline(xintercept = release_date, linetype = "dashed", color = "gray", size = 1) +
  labs(title = "YouTube Daily Views", x = "Date", y = "Total Daily Views") +
  scale_y_continuous(labels = scales::comma) 
# print needed with par(mfrow)
print(plot_google)
print(plot_youtube)
#### good practice reset to default
par(mfrow = c(1, 1))
```
## Google Search Trend Analysis

```{r echo=FALSE}
#### Install/Load libraries
### library(gtrendsR)  # Downloading was blocked after 3 downloads
library(tidyverse)
library(lubridate)
library(sf)
library(rnaturalearth)
library(gtrendsR)
library(countrycode)

```



```{r prepare and merge data manually downloaded from Google Trends}

#### Google Trends CSV-Dateien importieren und zusammenführen
#### Funktion zum Einlesen und Aufbereiten der CSV-Dateien

process_trends_csv <- function(filepath, country_code) {
  #### CSV einlesen und erste 2 Zeilen überspringen
  data <- read.csv(filepath, skip = 2, stringsAsFactors = FALSE)
  
  #### Spaltennamen einheitlich setzen
  colnames(data) <- c("date", "iPhone", "Tesla", "Rolex")

  #### <1 Werte in allen Trend-Spalten zu 0 ändern
  trend_columns <- c("iPhone", "Tesla", "Rolex")

  data <- data %>%
    mutate(across(all_of(trend_columns),
                  ~ as.numeric(str_replace(., "<1", "0"))))
    
  #### Datum konvertieren
  data$date <- as.Date(data$date)
  
  #### Country-Spalte hinzufügen
  data$country <- country_code
  
  return(data)
  }

```


```{r Alle drei CSV-Dateien einlesen}

trends_US <- process_trends_csv("./raw_data_from_google/multiTimeline_US.csv", "US")
trends_MX <- process_trends_csv("./raw_data_from_google/multiTimeline_MX.csv", "MX")
trends_DE <- process_trends_csv("./raw_data_from_google/multiTimeline_DE.csv", "DE")
trends_FR <- process_trends_csv("./raw_data_from_google/multiTimeline_FR.csv", "FR")
trends_JP <- process_trends_csv("./raw_data_from_google/multiTimeline_JP.csv", "JP")

#### Alle Daten zusammenführen und nach Datum und Land sortieren
trends_combined <- rbind(trends_US, trends_MX, trends_DE, trends_FR, trends_JP) %>%
  arrange(date, country)

##### Investigate the data
#### str(trends_combined)
#### head(trends_combined)
#### View(trends_combined)
#### summary(trends_combined)
#### table(trends_combined$country)


#### Daten in long format für Visualisierung
trends_long <- trends_combined %>%
  pivot_longer(
    cols = c(`iPhone`, `Tesla`, `Rolex`),
    names_to = "keyword",
    values_to = "hits"
  )

#### head(trends_long, 10)
#### View(trends_long)

```


```{r Speichern der kombinierten Daten}

#### write.csv(trends_combined, "trends_combined_wide.csv", row.names = FALSE)
#### write.csv(trends_long, "trends_combined_long.csv", row.names = FALSE)

```


```{r Facet line chart for all combinations of country & keyword}
#| output: true
#### Make the line graph with facets for all combinations

trends_long %>%
  ggplot(aes(x = date, y = hits)) +
  geom_line(colour = "darkblue", linewidth = .6) +
  labs(
    title = "Google Trends searches",
    y = "Relative importance on that day",
    caption = "Source: Google Trends (2017-11-14 to 2018-06-14)"
  ) +
  facet_wrap(~ country + keyword, nrow = 5, ncol = 3) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```


```{r Scatterplot Keyword 1 vs. Keyword 2 for all countries, echo=FALSE, message=FALSE}
#| output: true
#### Make the scatter plot with fitted lm line
ggplot(trends_combined, aes(x = iPhone, y = Rolex)) +
  geom_point(alpha = 0.6, size = 2, color = "darkblue") +
  geom_smooth(method = 'lm', se = TRUE) +
  labs(
    title = "Correlation between iPhone and Rolex in Google Trends searches",
    x = "Tesla",
    y = "iPhone",
    caption = "Source: Google Trends. Each point represents a day from 2017-11-14 to 2018-06-14."
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```


```{r Rolex bar chart with seasonality of Google Trends}
#| output: true
#### Get and prepare the data

#### res <- gtrends(
####   keyword = "Rolex",
####   geo     = "",
####   time    = "2018-01-01 2018-12-31")
#### 
#### write.csv(res$interest_over_time, "./raw_data_from_google/gtrends_Rolex_all_countries_year_2018.csv")

google_Rolex_2018 <- read.csv("./raw_data_from_google/gtrends_Rolex_all_countries_year_2018.csv")

plot_data <- google_Rolex_2018 %>% 
  select(date, hits) %>% 
  filter(date > "2018-01-01")

#### Prepare month axis
#### plot_data <- plot_data %>%
####   mutate(month_abbr = factor(month(month, label = TRUE, abbr = TRUE),
####                             levels = c("Nov", "Dez", "Jan", "Feb", "Mär", "Apr", "Mai", "Jun")))

#### Make the bar chart

ggplot(plot_data, aes(x = date, y = hits)) +
  geom_col(fill = "darkblue") +
  labs(
    title = "Rolex searches worldwide 2018 by week",
    x = "Weeks 2018",
    y = "Relative search interest (100 for month with most searches",
    caption = "Source: Google Trends (2018-01-01 to 2018-12-31)"
  ) +
  scale_y_continuous(limits = c(0, 100)) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),  #### Remove x-axis text labels
    axis.ticks.x = element_blank()   #### Remove x-axis ticks
  )

```
## Model Anlaysis (to be added)

## Youtube Videos Analysis

Scatterplot Tesla Youtube-views vs. Youtube-likes

```{r echo=FALSE, message=FALSE}
#| output: true
## Prepare data

plot_data <- aggregated_youtube_data_final %>% 
  filter(keyword == 'Rolex') %>% 
  select(trending_date, total_views, total_likes) %>% 
  mutate(engagement = total_likes / total_views)

## Plot the scatter plot

ggplot(plot_data, aes(x = total_views, y = engagement)) +
  geom_point(alpha = 0.6, size = 2, color = "darkblue") +
  scale_x_continuous(labels = scales::number, expand = expansion(mult = c(0, 0.1))) +
  geom_smooth(method = 'lm', se = FALSE) +
  labs(
    title = "Correlation between engagement with and views of Rolex YouTube videos",
    x = "Total Views",
    y = "Engagement: Likes / Views",
    caption = "Source: Google Trends. Each point represents a day from 2017-11-14 to 2018-06-14."
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

```

Barchart Brand comparison of YouTube-Engagement (i.e. likes / views and dislikes / views)

```{r echo=FALSE, message=FALSE}

## Prepare data

plot_data <- aggregated_youtube_data_final %>% 
  select(trending_date, keyword, total_views, total_likes, total_dislikes) %>% 
  group_by(keyword) %>% 
  summarise(total_views = sum(total_views),
            total_likes = sum(total_likes),
            total_dislikes = sum(total_dislikes)) %>% 
  mutate(like_ratio = total_likes / total_views,
         dislike_ratio = total_dislikes / total_views) %>% 
  mutate(keyword = factor(keyword, levels = c('iPhone', 'Tesla', 'Rolex')))

## Make the column plot

# ggplot(plot_data,aes(x = keyword, y = like_ratio)) +
#   geom_col(fill = "darkblue") +
#   geom_text(aes(label = sprintf("%.3f", like_ratio)),
#             vjust = -0.5, size = 3.2) +
#   scale_y_continuous(limits = c(0, 0.05)) +
#   theme_minimal() +
#   labs(
#     title = "Brand comparison of YouTube video engagement",
#     x = "Keywords",
#     y = "Engagement (total likes / total views)",
#     caption = "Source: Kaggle dataset Youtube (November 2017 to Juin 2018)"
#   )

## Make a combined column plot for like- and dislike-ratio

## Convert to long format: one column for metric type, one for value
plot_long <- plot_data %>%
  pivot_longer(cols = c(like_ratio, dislike_ratio),
               names_to = "metric",
               values_to = "ratio")

## Plot with grouped bars
ggplot(plot_long, aes(x = keyword, y = ratio, fill = metric)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = sprintf("%.3f", ratio)),
            vjust = -0.5, size = 3.2,
            position = position_dodge(width = 0.9)) +
  scale_y_continuous(limits = c(0, 0.05)) +
  scale_fill_manual(values = c("like_ratio" = "darkblue",
                               "dislike_ratio" = "firebrick")) +
  theme_minimal() +
  labs(
    title = "Brand comparison of YouTube video engagement",
    x = "Keywords",
    y = "Engagement (likes/dislikes per views)",
    fill = "Metric",
    caption = "Source: Kaggle dataset Youtube (Nov 2017 – Jun 2018)"
  )

```

Longevity of selected YouTube-videos

```{r echo=FALSE, message=FALSE}
#| output: true
## Prepare data

plot_data <- relevant_videos %>% 
  select(video_id, trending_date, views, keyword) %>% 
  group_by(video_id, trending_date, keyword) %>% 
  summarise(total_views = sum(views))

# plot_data %>% 
#   group_by(keyword) %>%
#   slice_max(total_views, n = 1) %>%
#   ungroup()

plot_data <- plot_data %>% 
  filter(video_id %in% c("RgpoRGq3oBs",	"WpqUOW19aJQ", "2fGXDFiFBhg"))

## Make the line graph

plot_data %>%
  ggplot(aes(x = trending_date, y = total_views, colour = keyword)) +
  geom_line(linewidth = .6) +
  labs(
    title = "YouTube most watched videos: Development over time",
    y = "Relative importance on that day",
    caption = "Source: Google Trends (2017-11-14 to 2018-06-14)"
  ) +
  theme_minimal()

```
## WordCloud

```{r echo=FALSE}
###  Wordcloud2.
#### Install packages
library(ragg)
library(systemfonts)
library(textshaping)

# For RStudio >= 1.4, go to Tools > Global Options > General > Graphics and set
# the Backend to AGG.

# View(systemfonts::system_fonts())
# Test Fonts. Use family name!

library(RColorBrewer)
library(wordcloud2)
library(tidyverse)
library(stopwords)
library(webshot2)
library(htmlwidgets)

```


```{r Prepare the Rolex data}

# Use only videos that have Rolex in the title or in the tags
rolex_df <- all_videos %>% 
  filter(
    str_detect(title, regex("rolex", ignore_case = TRUE)) |
    str_detect(tags, regex("rolex", ignore_case = TRUE))
  )

# Aggregate by video_id (sum views, keep tags)
rolex_df_aggregated <- rolex_df %>%
  group_by(video_id, tags) %>%
  summarise(total_views = sum(views, na.rm = TRUE), .groups = "drop")

# Split tags by "|" separator (creates multiple rows per video)
rolex_tags_split <- rolex_df_aggregated %>%
  separate_rows(tags, sep = "\\|")

# Clean individual tags
rolex_tags_cleaned <- rolex_tags_split %>%
  mutate(
    # Remove leading/trailing whitespace
    tags_clean = str_trim(tags),
    # Convert to lowercase for consistency
    tags_clean = str_to_lower(tags_clean),
    # Remove extra whitespace within tags
    tags_clean = str_squish(tags_clean)
  ) %>%
  # Remove empty tags and stopwords
  filter(
    tags_clean != "",
    !tags_clean %in% stopwords("en"),
    # Remove additional words you do not want to see in the cloud, namely 'Rolex'
    !tags_clean %in% c("video", "youtube", "content", "2017", "2018", "olexesh", "rolex")
  ) %>%
  # Keep only the cleaned tags column
  select(tag = tags_clean, total_views)

# View results
# head(rolex_tags_cleaned)
# cat("Total unique clean tags:", n_distinct(rolex_tags_cleaned$tag), "\n")

# aggregate to have unique tags
rolex_tags_cleaned_unique <- rolex_tags_cleaned %>%
  group_by(tag) %>%
  summarise(total_views = sum(total_views, na.rm = TRUE)) %>%
  arrange(desc(total_views))

# View results
# head(rolex_tags_cleaned_unique, 100)
```

Create the Rolex wordcloud

```{r}
#| output: true
# Define luxurious colors that match to the Rolex brand.

pal_rolex <- c("#228B22", "#0A1931", "#B09770", "#722F37", "#B36A5E", "#454545", "#006D77", "#4C3F91")

# Create the Rolex wordcloud

wc_Rolex <- wordcloud2(rolex_tags_cleaned_unique %>% filter(total_views > 100000),
           size = 0.3, rotateRatio = 0,
           minRotation = -pi/2, maxRotation = -pi/2,
           fontFamily = 'Garamond',
           fontWeight = 800,
           gridSize = 4, 
           shuffle = TRUE,
           shape = 'circle', ellipticity = 0.5,
           # color = palKH2[factor(df2$Word)])
           color = rep_len(pal_rolex, nrow(rolex_tags_cleaned_unique)))

wc_Rolex

## Export a png

## Step 1: Save as HTML in a known folder
# html_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_Rolex.html"
# saveWidget(wc_Rolex, file = html_file, selfcontained = TRUE)

## Step 2: Take PNG snapshot of that HTML
# png_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_Rolex.png"
# webshot2::webshot(html_file, png_file, vwidth = 800, vheight = 600)

```


```{r Prepare the iPhone data}
# Use only videos that have iphone in the title or in the tags
iphone_df <- all_videos %>% 
  filter(
    str_detect(title, regex("iphone", ignore_case = TRUE)) |
    str_detect(tags, regex("iphone", ignore_case = TRUE))
  )

# Aggregate by video_id (sum views, keep tags)
iphone_df_aggregated <- iphone_df %>%
  group_by(video_id, tags) %>%
  summarise(total_views = sum(views, na.rm = TRUE), .groups = "drop")

# Split tags by "|" separator (creates multiple rows per video)
iphone_tags_split <- iphone_df_aggregated %>%
  separate_rows(tags, sep = "\\|")

# Clean individual tags
iphone_tags_cleaned <- iphone_tags_split %>%
  mutate(
    # Remove leading/trailing whitespace
    tags_clean = str_trim(tags),
    # Convert to lowercase for consistency
    tags_clean = str_to_lower(tags_clean),
    # Remove extra whitespace within tags
    tags_clean = str_squish(tags_clean)
  ) %>%
  # Remove empty tags and stopwords
  filter(
    tags_clean != "",
    !tags_clean %in% stopwords("en"),
    # Remove additional words you do not want to see in the cloud, namely 'iPhone'
    !tags_clean %in% c("video", "youtube", "content", "2017", "2018", "iphone", "iphone x")
  ) %>%
  # Keep only the cleaned tags column
  select(tag = tags_clean, total_views)

# View results
# head(iphone_tags_cleaned)
# cat("Total unique clean tags:", n_distinct(iphone_tags_cleaned$tag), "\n")

# aggregate to have unique tags
iphone_tags_cleaned_unique <- iphone_tags_cleaned %>%
  group_by(tag) %>%
  summarise(total_views = sum(total_views, na.rm = TRUE)) %>%
  arrange(desc(total_views))

# View results
# head(iphone_tags_cleaned_unique, 100)
```

Create the iPhone wordcloud

```{r}
#| output: true
# Define colors that match to Apple and specifically to the iPhone X launched 2017/2018.

pal_iphone <- c("#FF375F", "#32D74B", "#0A84FF", "#FF9F0A", "#BF5AF2", "#1D1D1F", "#E4E4E2")

# iPhone wordcloud

wc_iPhone <- wordcloud2(iphone_tags_cleaned_unique %>% filter(total_views > 500000),
           size = 0.2, rotateRatio = 0,
           minRotation = -pi/2, maxRotation = -pi/2,
           fontFamily = 'Helvetica Neue LT Std',
           fontWeight = 800,
           gridSize = 4, 
           shuffle = TRUE,
           shape = 'circle', ellipticity = 0.5,
           # color = palKH2[factor(df2$Word)])
           color = rep_len(pal_iphone, nrow(iphone_tags_cleaned_unique)))

wc_iPhone

## Export a png

## Step 1: Save as HTML in a known folder
# html_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_iPhone.html"
# saveWidget(wc_iPhone, file = html_file, selfcontained = TRUE)

## Step 2: Take PNG snapshot of that HTML
# png_file <- "C:/Users/dhub1/Pictures/Screenshots/wordcloud_iPhone.png"
# webshot2::webshot(html_file, png_file, vwidth = 800, vheight = 600)
```